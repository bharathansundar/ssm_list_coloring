{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **General graph code**\n",
        "\n",
        "- seems to be the same as the tree case at face value\n",
        "- haven't actually updated all the optimizations in the way that i updated for the tree case yet\n",
        "- so look at that first"
      ],
      "metadata": {
        "id": "rwAs890ENOPL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "5_OBpgrg4IVt",
        "outputId": "5acd2c9c-fdc1-45cb-da85-db45f8cc6443"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "S_5_0_ab_1_5.pkl is done\n",
            "S_5_0_ac_1_5.pkl is done\n",
            "S_5_0_ad_1_5.pkl is done\n",
            "S_5_0_ae_1_5.pkl is done\n",
            "S_5_0_bc_1_5.pkl is done\n",
            "S_5_0_bd_1_5.pkl is done\n",
            "S_5_0_be_1_5.pkl is done\n",
            "S_5_0_cd_1_5.pkl is done\n",
            "S_5_0_ce_1_5.pkl is done\n",
            "S_5_0_de_1_5.pkl is done\n",
            "S_5_0_abc_1_5.pkl is done\n",
            "S_5_0_abd_1_5.pkl is done\n",
            "S_5_0_abe_1_5.pkl is done\n",
            "S_5_0_acd_1_5.pkl is done\n",
            "S_5_0_ace_1_5.pkl is done\n",
            "S_5_0_ade_1_5.pkl is done\n",
            "S_5_0_bcd_1_5.pkl is done\n",
            "S_5_0_bce_1_5.pkl is done\n",
            "S_5_0_bde_1_5.pkl is done\n",
            "S_5_0_cde_1_5.pkl is done\n",
            "S_5_0_abcd_1_5.pkl is done\n",
            "S_5_0_abce_1_5.pkl is done\n",
            "S_5_0_abde_1_5.pkl is done\n",
            "S_5_0_acde_1_5.pkl is done\n",
            "S_5_0_bcde_1_5.pkl is done\n",
            "S_5_0_abcde_1_5.pkl is done\n",
            "S_5_1_abc_1_5.pkl is done\n",
            "S_5_1_abd_1_5.pkl is done\n",
            "S_5_1_abe_1_5.pkl is done\n",
            "S_5_1_acd_1_5.pkl is done\n",
            "S_5_1_ace_1_5.pkl is done\n",
            "S_5_1_ade_1_5.pkl is done\n",
            "S_5_1_bcd_1_5.pkl is done\n",
            "S_5_1_bce_1_5.pkl is done\n",
            "S_5_1_bde_1_5.pkl is done\n",
            "S_5_1_cde_1_5.pkl is done\n",
            "S_5_2_abcd_1_5.pkl is done\n",
            "S_5_2_abce_1_5.pkl is done\n",
            "S_5_2_abde_1_5.pkl is done\n",
            "S_5_2_acde_1_5.pkl is done\n",
            "S_5_2_bcde_1_5.pkl is done\n",
            "S_5_3_abcde_1_5.pkl is done\n",
            "S_5_0_ab_2_5.pkl is done\n",
            "S_5_0_ac_2_5.pkl is done\n",
            "S_5_0_ad_2_5.pkl is done\n",
            "S_5_0_ae_2_5.pkl is done\n",
            "S_5_0_bc_2_5.pkl is done\n",
            "S_5_0_bd_2_5.pkl is done\n",
            "S_5_0_be_2_5.pkl is done\n",
            "S_5_0_cd_2_5.pkl is done\n",
            "S_5_0_ce_2_5.pkl is done\n",
            "S_5_0_de_2_5.pkl is done\n",
            "S_5_0_abc_2_5.pkl is done\n",
            "S_5_0_abd_2_5.pkl is done\n",
            "S_5_0_abe_2_5.pkl is done\n",
            "S_5_0_acd_2_5.pkl is done\n",
            "S_5_0_ace_2_5.pkl is done\n",
            "S_5_0_ade_2_5.pkl is done\n",
            "S_5_0_bcd_2_5.pkl is done\n",
            "S_5_0_bce_2_5.pkl is done\n",
            "S_5_0_bde_2_5.pkl is done\n",
            "S_5_0_cde_2_5.pkl is done\n",
            "S_5_0_abcd_2_5.pkl is done\n",
            "S_5_0_abce_2_5.pkl is done\n",
            "S_5_0_abde_2_5.pkl is done\n",
            "S_5_0_acde_2_5.pkl is done\n",
            "S_5_0_bcde_2_5.pkl is done\n",
            "S_5_0_abcde_2_5.pkl is done\n",
            "S_5_1_abc_2_5.pkl is done\n",
            "S_5_1_abd_2_5.pkl is done\n",
            "S_5_1_abe_2_5.pkl is done\n",
            "S_5_1_acd_2_5.pkl is done\n",
            "S_5_1_ace_2_5.pkl is done\n",
            "S_5_1_ade_2_5.pkl is done\n",
            "S_5_1_bcd_2_5.pkl is done\n",
            "S_5_1_bce_2_5.pkl is done\n",
            "S_5_1_bde_2_5.pkl is done\n",
            "S_5_1_cde_2_5.pkl is done\n",
            "S_5_2_abcd_2_5.pkl is done\n",
            "S_5_2_abce_2_5.pkl is done\n",
            "S_5_2_abde_2_5.pkl is done\n",
            "S_5_2_acde_2_5.pkl is done\n",
            "S_5_2_bcde_2_5.pkl is done\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import itertools\n",
        "import pickle\n",
        "from functools import lru_cache\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "\n",
        "num_samples = 5  # sample discretization of axis (num = 10 ---> 0.1 increments)\n",
        "q = 5  # of colors\n",
        "max_iter = 5  # max number of iterations to compute S for\n",
        "\n",
        "strategy = tf.distribute.MirroredStrategy()\n",
        "\n",
        "with strategy.scope():\n",
        "  def F(x):\n",
        "      # size (n, d, q) (n lists of d distributions over q colors)\n",
        "      # this is a working TF version of f\n",
        "\n",
        "      n = tf.shape(x)[0]\n",
        "      delt = tf.shape(x)[1]\n",
        "      num_colors = tf.shape(x)[2]\n",
        "\n",
        "      # Initialize Z and p_x\n",
        "      Z = tf.zeros([n], dtype=tf.float32)\n",
        "      p_x = tf.ones([n, num_colors], dtype=tf.float32)\n",
        "\n",
        "      # Compute p_x and Z\n",
        "      for j in range(num_colors):\n",
        "          # Compute product for each color j\n",
        "          p_x_j = tf.reduce_prod(1 - x[:, :, j], axis=1)\n",
        "\n",
        "          # Accumulate in Z\n",
        "          Z += p_x_j\n",
        "\n",
        "          # Update the j-th column of p_x\n",
        "          indices = tf.stack([tf.range(n), tf.fill([n], j)], axis=1)\n",
        "          p_x = tf.tensor_scatter_nd_update(p_x, indices, p_x_j)\n",
        "\n",
        "      # Normalize p_x by Z\n",
        "      p_x /= tf.expand_dims(Z, axis=1)\n",
        "\n",
        "      return p_x\n",
        "\n",
        "\n",
        "# add probability of 1 if the child below is fixed a color\n",
        "def generate_colors(q):\n",
        "    cols = ['a','b','c','d','e','f','g','h','i','j','k']\n",
        "    return cols[:q]\n",
        "\n",
        "def possible_pairings(q):\n",
        "    # valid types on a q coloring for the tree where d = q-2\n",
        "    colors = generate_colors(q)\n",
        "    subsets = []\n",
        "    for r in range(2, len(colors) + 1):\n",
        "        for combo in itertools.combinations(colors, r):\n",
        "            subsets.append(list(combo))\n",
        "\n",
        "    pairings = []\n",
        "    for sub in subsets:\n",
        "        pairings.append((0, sub))\n",
        "\n",
        "    for d in range(1, q-1):\n",
        "        subsets = itertools.combinations(colors, d + 2)\n",
        "        for subset in subsets:\n",
        "            pairings.append((d, list(subset)))\n",
        "\n",
        "    return pairings\n",
        "\n",
        "def get_sublist_indices(lst, sublist):\n",
        "    # returns a list of indices of the sublist values in the original list\n",
        "    original_list_set = set(lst)\n",
        "    indices = []\n",
        "    for element in sublist:\n",
        "        if element in original_list_set:\n",
        "            indices.append(lst.index(element))\n",
        "        else:\n",
        "            indices.append(None)\n",
        "    return indices\n",
        "\n",
        "def uniform(q, L):\n",
        "    # returns the uniform distribution over L (as a subset of the full list of q colors)\n",
        "    colors = generate_colors(q)\n",
        "    dist = tf.zeros([q], dtype=tf.float32)\n",
        "\n",
        "    indices = get_sublist_indices(colors, L)\n",
        "    prob = 1.0 / len(L)\n",
        "    for ind in indices:\n",
        "        if ind is not None:\n",
        "            dist = tf.tensor_scatter_nd_update(dist, [[ind]], [prob])\n",
        "\n",
        "    return tf.reshape(dist, (1, q))\n",
        "\n",
        "def generate_combos(delta, q):\n",
        "    # generates list of all possible configurations over the delta children\n",
        "    pairings = possible_pairings(q)\n",
        "    return list(itertools.product(pairings, repeat=delta))\n",
        "\n",
        "def discretize_simplex(q, subset, n, mass=1, dim=1):\n",
        "    # n is the number of parts we split the unit mass into, and divide that amongst subset\n",
        "    # discretizing the simplex on subset, a list of colors that is a subset of generate_colors(q)\n",
        "    d = len(subset)\n",
        "    full_list = generate_colors(q)\n",
        "\n",
        "    @lru_cache(None)\n",
        "    def generate_points(mass, dim):\n",
        "        points = []\n",
        "        if dim == d:\n",
        "            return [[mass]]\n",
        "\n",
        "        values = tf.range(0, mass + 1. / n, 1. / n).numpy()\n",
        "        for v in values:\n",
        "            suffixes = generate_points(mass - v, dim + 1)\n",
        "            points.extend([[v] + s for s in suffixes])\n",
        "\n",
        "        return points\n",
        "\n",
        "    points = generate_points(mass, dim)\n",
        "\n",
        "    if dim == 1:\n",
        "        # Embed points in 4D space with zeros in the appropriate positions\n",
        "        points_qd = tf.zeros([len(points), q], dtype=tf.float32)\n",
        "        indices = [full_list.index(item) for item in subset]\n",
        "\n",
        "        for i, point in enumerate(points):\n",
        "            for j, index in enumerate(indices):\n",
        "                points_qd = tf.tensor_scatter_nd_update(points_qd, [[i, index]], [point[j]])\n",
        "\n",
        "        return points_qd\n",
        "    else:\n",
        "        return points\n",
        "\n",
        "def find_nearest_vectors(vectors, probs):\n",
        "    # Calculate the distances from each prob_vector to each vectors\n",
        "    distances = tf.norm(vectors[:, tf.newaxis] - probs, axis=2)\n",
        "\n",
        "    # Find the indices of the nearest vectors\n",
        "    nearest_indices = tf.argmin(distances, axis=0)\n",
        "\n",
        "    # Select the nearest vectors based on the indices\n",
        "    nearest_vectors = tf.gather(vectors, nearest_indices)\n",
        "\n",
        "    return nearest_vectors.numpy()\n",
        "\n",
        "\n",
        "def S(q, delta, L, r):\n",
        "    # this the mega recursion for general delta and q\n",
        "    # delta is likely to be q - 2 exactly, for q >= 4\n",
        "\n",
        "    discretized = discretize_simplex(q, L, num_samples)\n",
        "\n",
        "    if delta == 0:\n",
        "        return uniform(q, L)\n",
        "\n",
        "    if r == 1:\n",
        "        return discretized\n",
        "\n",
        "    combos = generate_combos(delta, q)  # gives me possible combinations over the delta children\n",
        "    diff = (q-2) - delta\n",
        "    zros = [[0 for i in range(q)]]\n",
        "\n",
        "    final_probabilities = tf.zeros([0, q], dtype=tf.float32)\n",
        "\n",
        "\n",
        "    types = possible_pairings(q)\n",
        "    temps = []\n",
        "\n",
        "    for dlt1, lst1 in types:\n",
        "      col = \"\".join(lst1)\n",
        "      name = f\"S_{q}_{dlt1}_{col}_{r-1}_{num_samples}.pkl\"\n",
        "\n",
        "      with open(name, 'rb') as file:\n",
        "        temp = pickle.load(file)\n",
        "\n",
        "      temps.append(temp)\n",
        "\n",
        "    total = tf.concat(temps, axis=0)\n",
        "    filtered, inds = tf.raw_ops.UniqueV2(x=total, axis=[0])\n",
        "\n",
        "\n",
        "    for dlt2, lst2 in types:\n",
        "      col = \"\".join(lst2)\n",
        "      name = f\"S_{q}_{dlt2}_{col}_{r-1}_{num_samples}.pkl\"\n",
        "\n",
        "      with open(name, 'rb') as file:\n",
        "        temp2 = pickle.load(file)\n",
        "\n",
        "      sets = []\n",
        "      sets.append(temp2)\n",
        "\n",
        "      for i in range(delta-1):\n",
        "        sets.append(filtered)\n",
        "\n",
        "      for i in range(diff):\n",
        "        sets.append(zros)\n",
        "\n",
        "      collection = list(itertools.product(*sets))\n",
        "\n",
        "      distributions = [list(dis) for dis in collection]  # convert everything into lists of lists of lists (shape n,d,q)\n",
        "\n",
        "      distributions_array = tf.convert_to_tensor(distributions, dtype=tf.float32)\n",
        "\n",
        "      probs = F(distributions_array)  # outputs list of possible probabilities\n",
        "      projs = find_nearest_vectors(discretized, tf.convert_to_tensor(probs, dtype=tf.float32))  # the projection takes in vectors\n",
        "\n",
        "      final_probabilities = tf.concat([final_probabilities, projs], axis=0)\n",
        "\n",
        "    final_probs, idx = tf.raw_ops.UniqueV2(x=final_probabilities, axis=[0])\n",
        "\n",
        "    return final_probs\n",
        "\n",
        "# Computing S for different r, accessing stored files.\n",
        "for r in range(1, max_iter + 1):\n",
        "    for tup in possible_pairings(q):  # evaluate S for all value combinations\n",
        "        delta, L = tup\n",
        "        colors = \"\".join(L)\n",
        "        name = f\"S_{q}_{delta}_{colors}_{r}_{num_samples}.pkl\"\n",
        "\n",
        "        lst = S(q, delta, L, r)\n",
        "\n",
        "        with open(name, 'wb') as file:\n",
        "            pickle.dump(lst, file)\n",
        "\n",
        "        print(name + \" is done\")\n",
        "\n",
        "# Code for visualizing the first two coordinates\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "q_ = 4\n",
        "delt = 2\n",
        "lst = ['a','b','c','d']\n",
        "\n",
        "cols = \"\".join(lst)\n",
        "\n",
        "# comparing to the original discretization at r = 1\n",
        "name_orig = f\"S_{q_}_{delt}_{cols}_{1}_{num_samples}.pkl\"\n",
        "with open(name_orig, 'rb') as file:\n",
        "    S_orig = pickle.load(file)\n",
        "\n",
        "\n",
        "name = f\"S_{q_}_{delt}_{cols}_{max_iter}_{num_samples}.pkl\"\n",
        "with open(name, 'rb') as file:\n",
        "    S_r = pickle.load(file)\n",
        "\n",
        "\n",
        "# Extract p1 and p2 values\n",
        "p1_orig = [p[0] for p in S_orig]\n",
        "p2_orig = [p[1] for p in S_orig]\n",
        "\n",
        "p1_values = [q[0] for q in S_r]\n",
        "p2_values = [q[1] for q in S_r]\n",
        "\n",
        "# Create the plot\n",
        "plt.figure(figsize=(10,10))\n",
        "plt.scatter(p1_orig, p2_orig, c='green', label='p1 (r=1), p2 (r=1)', s=12)\n",
        "plt.scatter(p1_values, p2_values, c='red', label=f'p1 (r={max_iter}), p2 (r={max_iter})', s=15)\n",
        "\n",
        "plt.xlabel('p1')\n",
        "plt.ylabel('p2')\n",
        "plt.title(f'Probability Vectors [p1, p2] for S({delt}, {lst}, {max_iter}), initially using {num_samples} points to discretize')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "# Set axis limits to show the whole simplex\n",
        "plt.xlim(-0.1, 1.1)\n",
        "plt.ylim(-.1, 1.1)\n",
        "\n",
        "# Show plot\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "erm7MQylNMsp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BAXZ4lizNpvs"
      },
      "outputs": [],
      "source": [
        "# Jacobian calculations\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1kvSPK19ANbb"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pPuexSdMAa3U",
        "outputId": "9d391f6b-9858-44f5-caf3-0df72a61fcca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(1, 4)\n",
            "(1, 4)\n",
            "(1, 4)\n",
            "(1, 4)\n",
            "(1, 4)\n",
            "(1, 4)\n",
            "(1, 4)\n",
            "(1, 4)\n",
            "(1, 4)\n",
            "(1, 4)\n",
            "(1, 4)\n",
            "(6, 4)\n",
            "(6, 4)\n",
            "(6, 4)\n",
            "(6, 4)\n",
            "(56, 4)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "TensorShape([91, 4])"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pairs = possible_pairings(q)\n",
        "temps = []\n",
        "\n",
        "for dlt, lst in pairs:\n",
        "  col = \"\".join(lst)\n",
        "  name = f\"S_{q}_{dlt}_{col}_{3}_{num_samples}.pkl\"\n",
        "\n",
        "  with open(name, 'rb') as file:\n",
        "    temp = pickle.load(file)\n",
        "  temps.append(temp)\n",
        "\n",
        "  print(temp.shape)\n",
        "\n",
        "total = tf.concat(temps, axis=0)\n",
        "filtered, inds = tf.raw_ops.UniqueV2(x=total, axis=[0])\n",
        "\n",
        "filtered.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NPUsSuwqv-k5"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tDtRBhrP2FY2"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}